{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöó Global Road Accidents - Classification Analysis\n",
    "## Complete Classification Task - UN SDG 3: Good Health and Well-being\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Module:** 5CS037  \n",
    "**Due Date:** February 10, 2026\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading and Understanding](#1)\n",
    "2. [Exploratory Data Analysis (EDA)](#2)\n",
    "3. [Data Preprocessing](#3)\n",
    "4. [Neural Network Model](#4)\n",
    "5. [Classical ML Models](#5)\n",
    "6. [Hyperparameter Optimization](#6)\n",
    "7. [Feature Selection](#7)\n",
    "8. [Final Models and Comparison](#8)\n",
    "9. [Conclusion](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
    "\n",
    "# Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Upload Dataset to Google Colab\n",
    "\n",
    "**Instructions:**\n",
    "1. Upload the `road_accident_dataset.csv` file using the file upload button\n",
    "2. Or upload to Google Drive and mount it\n",
    "\n",
    "Uncomment the method you prefer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 1: Direct file upload (recommended for simplicity)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "# The file will be available in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 2: Google Drive (if file is already in Drive)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# df = pd.read_csv('/content/drive/MyDrive/road_accident_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='1'></a>\n",
    "# 1. Data Loading and Understanding\n",
    "\n",
    "### 1.1 Dataset Information\n",
    "\n",
    "**Dataset Name:** Global Road Accidents Dataset  \n",
    "**Source:** Synthetic dataset for educational purposes  \n",
    "**Created:** 2024  \n",
    "**Access:** Provided for university coursework\n",
    "\n",
    "**UN SDG Alignment:** This dataset aligns with **UN Sustainable Development Goal 3: Good Health and Well-being**, specifically Target 3.6 which aims to halve the number of global deaths and injuries from road traffic accidents by 2030.\n",
    "\n",
    "**Research Questions:**\n",
    "1. What factors contribute most to severe road accidents?\n",
    "2. Can we predict accident severity based on environmental and driver characteristics?\n",
    "3. How do weather conditions and road types impact accident severity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('road_accident_dataset.csv')\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first few rows\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET PREVIEW\")\n",
    "print(\"=\" * 80)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target variable\n",
    "print(\"=\" * 80)\n",
    "print(\"TARGET VARIABLE: Accident Severity\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTarget Variable Distribution:\")\n",
    "print(df['Accident Severity'].value_counts())\n",
    "print(f\"\\nTarget Variable Percentages:\")\n",
    "print(df['Accident Severity'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset quality assessment\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. MISSING VALUES:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"   ‚úÖ No missing values detected\")\n",
    "else:\n",
    "    print(missing[missing > 0])\n",
    "\n",
    "print(\"\\n2. DUPLICATE ROWS:\")\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"   Total duplicates: {duplicates}\")\n",
    "\n",
    "print(\"\\n3. DATA TYPES:\")\n",
    "print(f\"   Numerical columns: {df.select_dtypes(include=['float64', 'int64']).shape[1]}\")\n",
    "print(f\"   Categorical columns: {df.select_dtypes(include=['object']).shape[1]}\")\n",
    "\n",
    "print(\"\\n4. CLASS BALANCE:\")\n",
    "print(\"   Target variable distribution shows\", end=\" \")\n",
    "balance_ratio = df['Accident Severity'].value_counts().min() / df['Accident Severity'].value_counts().max()\n",
    "if balance_ratio > 0.7:\n",
    "    print(\"‚úÖ relatively balanced classes\")\n",
    "elif balance_ratio > 0.3:\n",
    "    print(\"‚ö†Ô∏è moderate imbalance\")\n",
    "else:\n",
    "    print(\"‚ùå significant class imbalance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Feature Descriptions\n",
    "\n",
    "**Categorical Features:**\n",
    "- Country, Month, Day of Week, Time of Day, Urban/Rural, Road Type\n",
    "- Weather Conditions, Driver Age Group, Driver Gender, Vehicle Condition\n",
    "- Road Condition, Accident Cause, Region\n",
    "\n",
    "**Numerical Features:**\n",
    "- Year, Visibility Level, Number of Vehicles Involved, Speed Limit\n",
    "- Driver Alcohol Level, Number of Injuries, Number of Fatalities\n",
    "- Emergency Response Time, Traffic Volume, Medical Cost, Economic Loss\n",
    "- Population Density\n",
    "\n",
    "**Target Variable:**\n",
    "- Accident Severity (Minor, Moderate, Severe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='2'></a>\n",
    "# 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 2.1 Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of numerical features\n",
    "print(\"=\" * 80)\n",
    "print(\"STATISTICAL SUMMARY - NUMERICAL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of categorical features\n",
    "print(\"=\" * 80)\n",
    "print(\"STATISTICAL SUMMARY - CATEGORICAL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target variable distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "severity_counts = df['Accident Severity'].value_counts()\n",
    "axes[0].bar(severity_counts.index, severity_counts.values, color=['#2ecc71', '#f39c12', '#e74c3c'])\n",
    "axes[0].set_title('Accident Severity Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Severity Level')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels\n",
    "for i, v in enumerate(severity_counts.values):\n",
    "    axes[0].text(i, v + 100, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "axes[1].pie(severity_counts.values, labels=severity_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, startangle=90)\n",
    "axes[1].set_title('Accident Severity Proportion', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Insight: The dataset shows distribution across three severity levels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key numerical features for analysis\n",
    "numerical_features = ['Speed Limit', 'Driver Alcohol Level', 'Number of Vehicles Involved', \n",
    "                     'Number of Injuries', 'Number of Fatalities', 'Emergency Response Time',\n",
    "                     'Traffic Volume', 'Visibility Level']\n",
    "\n",
    "# Distribution plots\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_features):\n",
    "    axes[idx].hist(df[col], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{col}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Value')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Insight: Numerical features show various distributions - some are normally distributed while others are skewed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots to identify outliers\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_features):\n",
    "    axes[idx].boxplot(df[col].dropna())\n",
    "    axes[idx].set_title(f'{col}', fontweight='bold')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Insight: Box plots reveal potential outliers in several features, which is normal for accident data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key categorical features\n",
    "categorical_features = ['Weather Conditions', 'Road Type', 'Urban/Rural', 'Time of Day', \n",
    "                       'Driver Age Group', 'Accident Cause']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_features):\n",
    "    value_counts = df[col].value_counts()\n",
    "    axes[idx].barh(value_counts.index, value_counts.values, color='coral')\n",
    "    axes[idx].set_title(f'{col}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Count')\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Insight: Categorical features show varied distributions across different categories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical columns for correlation\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1)\n",
    "plt.title('Correlation Matrix - Numerical Features', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Insight: Some features show strong correlations (e.g., injuries and fatalities), which is expected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Feature vs Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key features against accident severity\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "features_to_analyze = ['Weather Conditions', 'Road Type', 'Driver Age Group', \n",
    "                       'Time of Day', 'Urban/Rural', 'Accident Cause']\n",
    "\n",
    "for idx, feature in enumerate(features_to_analyze):\n",
    "    crosstab = pd.crosstab(df[feature], df['Accident Severity'], normalize='index') * 100\n",
    "    crosstab.plot(kind='bar', ax=axes[idx], color=['#2ecc71', '#f39c12', '#e74c3c'])\n",
    "    axes[idx].set_title(f'{feature} vs Accident Severity', fontweight='bold')\n",
    "    axes[idx].set_xlabel('')\n",
    "    axes[idx].set_ylabel('Percentage (%)')\n",
    "    axes[idx].legend(title='Severity', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Insight: Different features show varying patterns in relation to accident severity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='3'></a>\n",
    "# 3. Data Preprocessing\n",
    "\n",
    "### 3.1 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Encode target variable\n",
    "target_encoder = LabelEncoder()\n",
    "df_processed['Accident Severity Encoded'] = target_encoder.fit_transform(df_processed['Accident Severity'])\n",
    "\n",
    "print(\"Target Variable Encoding:\")\n",
    "for i, label in enumerate(target_encoder.classes_):\n",
    "    print(f\"  {label} ‚Üí {i}\")\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols.remove('Accident Severity')  # Remove original target\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[f'{col}_Encoded'] = le.fit_transform(df_processed[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"\\n‚úÖ Encoded {len(categorical_cols)} categorical features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Selection for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "# Include encoded categorical features and numerical features\n",
    "encoded_cols = [col for col in df_processed.columns if col.endswith('_Encoded') and col != 'Accident Severity Encoded']\n",
    "numerical_cols_for_model = ['Year', 'Visibility Level', 'Number of Vehicles Involved', 'Speed Limit',\n",
    "                            'Driver Alcohol Level', 'Driver Fatigue', 'Pedestrians Involved',\n",
    "                            'Cyclists Involved', 'Number of Injuries', 'Number of Fatalities',\n",
    "                            'Emergency Response Time', 'Traffic Volume', 'Medical Cost',\n",
    "                            'Economic Loss', 'Population Density']\n",
    "\n",
    "feature_columns = encoded_cols + numerical_cols_for_model\n",
    "\n",
    "X = df_processed[feature_columns]\n",
    "y = df_processed['Accident Severity Encoded']\n",
    "\n",
    "print(f\"Features selected for modeling: {len(feature_columns)}\")\n",
    "print(f\"  - Encoded categorical: {len(encoded_cols)}\")\n",
    "print(f\"  - Numerical: {len(numerical_cols_for_model)}\")\n",
    "print(f\"\\nDataset shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Train-Test Split Results:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (important for Neural Network and some other algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Features scaled using StandardScaler\")\n",
    "print(f\"\\nScaled training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='4'></a>\n",
    "# 4. Neural Network Model (Task 1)\n",
    "\n",
    "### 4.1 Model Architecture\n",
    "\n",
    "**Network Architecture:**\n",
    "- Input Layer: Features from the dataset\n",
    "- Hidden Layer 1: 128 neurons, ReLU activation\n",
    "- Hidden Layer 2: 64 neurons, ReLU activation\n",
    "- Hidden Layer 3: 32 neurons, ReLU activation\n",
    "- Output Layer: 3 neurons (for 3 classes), Softmax activation\n",
    "\n",
    "**Training Configuration:**\n",
    "- Optimizer: Adam\n",
    "- Loss Function: Log Loss (Cross-Entropy)\n",
    "- Max Iterations: 500\n",
    "- Learning Rate: Adaptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train Neural Network\n",
    "print(\"üß† Training Neural Network...\\n\")\n",
    "\n",
    "nn_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "nn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úÖ Neural Network training completed!\")\n",
    "print(f\"\\nNumber of iterations: {nn_model.n_iter_}\")\n",
    "print(f\"Final loss: {nn_model.loss_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Neural Network Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred_nn = nn_model.predict(X_train_scaled)\n",
    "y_test_pred_nn = nn_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"=\" * 80)\n",
    "print(\"NEURAL NETWORK PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Training metrics\n",
    "train_acc_nn = accuracy_score(y_train, y_train_pred_nn)\n",
    "train_precision_nn = precision_score(y_train, y_train_pred_nn, average='weighted')\n",
    "train_recall_nn = recall_score(y_train, y_train_pred_nn, average='weighted')\n",
    "train_f1_nn = f1_score(y_train, y_train_pred_nn, average='weighted')\n",
    "\n",
    "print(\"\\nüìä TRAINING SET PERFORMANCE:\")\n",
    "print(f\"   Accuracy:  {train_acc_nn:.4f}\")\n",
    "print(f\"   Precision: {train_precision_nn:.4f}\")\n",
    "print(f\"   Recall:    {train_recall_nn:.4f}\")\n",
    "print(f\"   F1-Score:  {train_f1_nn:.4f}\")\n",
    "\n",
    "# Test metrics\n",
    "test_acc_nn = accuracy_score(y_test, y_test_pred_nn)\n",
    "test_precision_nn = precision_score(y_test, y_test_pred_nn, average='weighted')\n",
    "test_recall_nn = recall_score(y_test, y_test_pred_nn, average='weighted')\n",
    "test_f1_nn = f1_score(y_test, y_test_pred_nn, average='weighted')\n",
    "\n",
    "print(\"\\nüìä TEST SET PERFORMANCE:\")\n",
    "print(f\"   Accuracy:  {test_acc_nn:.4f}\")\n",
    "print(f\"   Precision: {test_precision_nn:.4f}\")\n",
    "print(f\"   Recall:    {test_recall_nn:.4f}\")\n",
    "print(f\"   F1-Score:  {test_f1_nn:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìã DETAILED CLASSIFICATION REPORT (TEST SET):\")\n",
    "print(classification_report(y_test, y_test_pred_nn, target_names=target_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm_nn = confusion_matrix(y_test, y_test_pred_nn)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_encoder.classes_, \n",
    "            yticklabels=target_encoder.classes_)\n",
    "plt.title('Neural Network - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Insight: The confusion matrix shows how well the neural network classifies each severity level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='5'></a>\n",
    "# 5. Classical ML Models (Task 2)\n",
    "\n",
    "## Model 1: Logistic Regression\n",
    "## Model 2: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train Logistic Regression\n",
    "print(\"üìä Training Logistic Regression...\\n\")\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úÖ Logistic Regression training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Logistic Regression\n",
    "y_train_pred_lr = lr_model.predict(X_train_scaled)\n",
    "y_test_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOGISTIC REGRESSION PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Training metrics\n",
    "train_acc_lr = accuracy_score(y_train, y_train_pred_lr)\n",
    "train_precision_lr = precision_score(y_train, y_train_pred_lr, average='weighted')\n",
    "train_recall_lr = recall_score(y_train, y_train_pred_lr, average='weighted')\n",
    "train_f1_lr = f1_score(y_train, y_train_pred_lr, average='weighted')\n",
    "\n",
    "print(\"\\nüìä TRAINING SET PERFORMANCE:\")\n",
    "print(f\"   Accuracy:  {train_acc_lr:.4f}\")\n",
    "print(f\"   Precision: {train_precision_lr:.4f}\")\n",
    "print(f\"   Recall:    {train_recall_lr:.4f}\")\n",
    "print(f\"   F1-Score:  {train_f1_lr:.4f}\")\n",
    "\n",
    "# Test metrics\n",
    "test_acc_lr = accuracy_score(y_test, y_test_pred_lr)\n",
    "test_precision_lr = precision_score(y_test, y_test_pred_lr, average='weighted')\n",
    "test_recall_lr = recall_score(y_test, y_test_pred_lr, average='weighted')\n",
    "test_f1_lr = f1_score(y_test, y_test_pred_lr, average='weighted')\n",
    "\n",
    "print(\"\\nüìä TEST SET PERFORMANCE:\")\n",
    "print(f\"   Accuracy:  {test_acc_lr:.4f}\")\n",
    "print(f\"   Precision: {test_precision_lr:.4f}\")\n",
    "print(f\"   Recall:    {test_recall_lr:.4f}\")\n",
    "print(f\"   F1-Score:  {test_f1_lr:.4f}\")\n",
    "\n",
    "print(\"\\nüìã DETAILED CLASSIFICATION REPORT (TEST SET):\")\n",
    "print(classification_report(y_test, y_test_pred_lr, target_names=target_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train Random Forest\n",
    "print(\"üå≤ Training Random Forest Classifier...\\n\")\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Random Forest training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RANDOM FOREST PERFORMANCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Training metrics\n",
    "train_acc_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "train_precision_rf = precision_score(y_train, y_train_pred_rf, average='weighted')\n",
    "train_recall_rf = recall_score(y_train, y_train_pred_rf, average='weighted')\n",
    "train_f1_rf = f1_score(y_train, y_train_pred_rf, average='weighted')\n",
    "\n",
    "print(\"\\nüìä TRAINING SET PERFORMANCE:\")\n",
    "print(f\"   Accuracy:  {train_acc_rf:.4f}\")\n",
    "print(f\"   Precision: {train_precision_rf:.4f}\")\n",
    "print(f\"   Recall:    {train_recall_rf:.4f}\")\n",
    "print(f\"   F1-Score:  {train_f1_rf:.4f}\")\n",
    "\n",
    "# Test metrics\n",
    "test_acc_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "test_precision_rf = precision_score(y_test, y_test_pred_rf, average='weighted')\n",
    "test_recall_rf = recall_score(y_test, y_test_pred_rf, average='weighted')\n",
    "test_f1_rf = f1_score(y_test, y_test_pred_rf, average='weighted')\n",
    "\n",
    "print(\"\\nüìä TEST SET PERFORMANCE:\")\n",
    "print(f\"   Accuracy:  {test_acc_rf:.4f}\")\n",
    "print(f\"   Precision: {test_precision_rf:.4f}\")\n",
    "print(f\"   Recall:    {test_recall_rf:.4f}\")\n",
    "print(f\"   F1-Score:  {test_f1_rf:.4f}\")\n",
    "\n",
    "print(\"\\nüìã DETAILED CLASSIFICATION REPORT (TEST SET):\")\n",
    "print(classification_report(y_test, y_test_pred_rf, target_names=target_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Initial Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare initial models\n",
    "comparison_data = {\n",
    "    'Model': ['Logistic Regression', 'Random Forest'],\n",
    "    'Train Accuracy': [train_acc_lr, train_acc_rf],\n",
    "    'Test Accuracy': [test_acc_lr, test_acc_rf],\n",
    "    'Test Precision': [test_precision_lr, test_precision_rf],\n",
    "    'Test Recall': [test_recall_lr, test_recall_rf],\n",
    "    'Test F1-Score': [test_f1_lr, test_f1_rf]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"=\" * 80)\n",
    "print(\"INITIAL MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(comparison_df['Model']))\n",
    "width = 0.15\n",
    "\n",
    "metrics = ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1-Score']\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(x + i*width, comparison_df[metric], width, label=metric, color=colors[i])\n",
    "\n",
    "ax.set_xlabel('Model', fontweight='bold')\n",
    "ax.set_ylabel('Score', fontweight='bold')\n",
    "ax.set_title('Initial Model Performance Comparison', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(comparison_df['Model'])\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Initial Conclusion: Both models show competitive performance. Optimization will help improve them further.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='6'></a>\n",
    "# 6. Hyperparameter Optimization (Task 4)\n",
    "\n",
    "### 6.1 Logistic Regression Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for Logistic Regression\n",
    "lr_param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs', 'saga'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "print(\"üîç Performing Hyperparameter Tuning for Logistic Regression...\\n\")\n",
    "\n",
    "# Perform GridSearchCV\n",
    "lr_grid_search = GridSearchCV(\n",
    "    LogisticRegression(random_state=42),\n",
    "    lr_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Hyperparameter tuning completed!\")\n",
    "print(f\"\\nBest parameters: {lr_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {lr_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Random Forest Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(\"üîç Performing Hyperparameter Tuning for Random Forest...\\n\")\n",
    "\n",
    "# Perform GridSearchCV\n",
    "rf_grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Hyperparameter tuning completed!\")\n",
    "print(f\"\\nBest parameters: {rf_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {rf_grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Summary of Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "tuning_summary = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest'],\n",
    "    'Best CV Score': [\n",
    "        f\"{lr_grid_search.best_score_:.4f}\",\n",
    "        f\"{rf_grid_search.best_score_:.4f}\"\n",
    "    ],\n",
    "    'Best Parameters': [\n",
    "        str(lr_grid_search.best_params_),\n",
    "        str(rf_grid_search.best_params_)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(tuning_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='7'></a>\n",
    "# 7. Feature Selection (Task 5)\n",
    "\n",
    "### 7.1 Feature Selection Using SelectKBest (Filter Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SelectKBest for feature selection\n",
    "k_best = 20  # Select top 20 features\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_features_mask = selector.get_support()\n",
    "selected_features = X.columns[selected_features_mask].tolist()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"FEATURE SELECTION - SelectKBest (Top {k_best} Features)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSelected {len(selected_features)} features:\\n\")\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "# Get feature scores\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Score': selector.scores_,\n",
    "    'Selected': selected_features_mask\n",
    "}).sort_values('Score', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 features by score:\")\n",
    "print(feature_scores.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "top_20_features = feature_scores.head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(top_20_features)), top_20_features['Score'], color='skyblue')\n",
    "plt.yticks(range(len(top_20_features)), top_20_features['Feature'])\n",
    "plt.xlabel('Feature Score', fontweight='bold')\n",
    "plt.title('Top 20 Features by Importance Score', fontweight='bold', fontsize=14)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Justification: SelectKBest uses ANOVA F-statistic to identify features with the\")\n",
    "print(\"   strongest relationship to the target variable. The selected features capture the\")\n",
    "print(\"   most predictive information while reducing model complexity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='8'></a>\n",
    "# 8. Final Models and Comparison (Task 6)\n",
    "\n",
    "### 8.1 Build Final Models with Optimal Parameters and Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the selected features\n",
    "scaler_final = StandardScaler()\n",
    "X_train_selected_scaled = scaler_final.fit_transform(X_train_selected)\n",
    "X_test_selected_scaled = scaler_final.transform(X_test_selected)\n",
    "\n",
    "print(\"‚úÖ Feature scaling completed for selected features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Logistic Regression with optimized parameters and selected features\n",
    "print(\"üéØ Training Final Logistic Regression Model...\\n\")\n",
    "\n",
    "final_lr_model = LogisticRegression(\n",
    "    **lr_grid_search.best_params_,\n",
    "    random_state=42\n",
    ")\n",
    "final_lr_model.fit(X_train_selected_scaled, y_train)\n",
    "\n",
    "# Cross-validation score\n",
    "lr_cv_scores = cross_val_score(final_lr_model, X_train_selected_scaled, y_train, cv=5)\n",
    "lr_cv_mean = lr_cv_scores.mean()\n",
    "\n",
    "# Predictions\n",
    "y_test_pred_final_lr = final_lr_model.predict(X_test_selected_scaled)\n",
    "\n",
    "# Metrics\n",
    "final_lr_acc = accuracy_score(y_test, y_test_pred_final_lr)\n",
    "final_lr_precision = precision_score(y_test, y_test_pred_final_lr, average='weighted')\n",
    "final_lr_recall = recall_score(y_test, y_test_pred_final_lr, average='weighted')\n",
    "final_lr_f1 = f1_score(y_test, y_test_pred_final_lr, average='weighted')\n",
    "\n",
    "print(\"‚úÖ Final Logistic Regression model trained!\")\n",
    "print(f\"   CV Score: {lr_cv_mean:.4f}\")\n",
    "print(f\"   Test Accuracy: {final_lr_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Random Forest with optimized parameters and selected features\n",
    "print(\"üéØ Training Final Random Forest Model...\\n\")\n",
    "\n",
    "final_rf_model = RandomForestClassifier(\n",
    "    **rf_grid_search.best_params_,\n",
    "    random_state=42\n",
    ")\n",
    "final_rf_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Cross-validation score\n",
    "rf_cv_scores = cross_val_score(final_rf_model, X_train_selected, y_train, cv=5)\n",
    "rf_cv_mean = rf_cv_scores.mean()\n",
    "\n",
    "# Predictions\n",
    "y_test_pred_final_rf = final_rf_model.predict(X_test_selected)\n",
    "\n",
    "# Metrics\n",
    "final_rf_acc = accuracy_score(y_test, y_test_pred_final_rf)\n",
    "final_rf_precision = precision_score(y_test, y_test_pred_final_rf, average='weighted')\n",
    "final_rf_recall = recall_score(y_test, y_test_pred_final_rf, average='weighted')\n",
    "final_rf_f1 = f1_score(y_test, y_test_pred_final_rf, average='weighted')\n",
    "\n",
    "print(\"‚úÖ Final Random Forest model trained!\")\n",
    "print(f\"   CV Score: {rf_cv_mean:.4f}\")\n",
    "print(f\"   Test Accuracy: {final_rf_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Final Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest'],\n",
    "    'Features': [k_best, k_best],\n",
    "    'CV Score': [f\"{lr_cv_mean:.4f}\", f\"{rf_cv_mean:.4f}\"],\n",
    "    'Accuracy': [f\"{final_lr_acc:.4f}\", f\"{final_rf_acc:.4f}\"],\n",
    "    'Precision': [f\"{final_lr_precision:.4f}\", f\"{final_rf_precision:.4f}\"],\n",
    "    'Recall': [f\"{final_lr_recall:.4f}\", f\"{final_rf_recall:.4f}\"],\n",
    "    'F1-Score': [f\"{final_lr_f1:.4f}\", f\"{final_rf_f1:.4f}\"]\n",
    "})\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(final_comparison.to_string(index=False))\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = 'Random Forest' if final_rf_f1 > final_lr_f1 else 'Logistic Regression'\n",
    "print(f\"\\nüèÜ Best performing model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "models = ['Logistic Regression', 'Random Forest']\n",
    "metrics_values = {\n",
    "    'Accuracy': [final_lr_acc, final_rf_acc],\n",
    "    'Precision': [final_lr_precision, final_rf_precision],\n",
    "    'Recall': [final_lr_recall, final_rf_recall],\n",
    "    'F1-Score': [final_lr_f1, final_rf_f1]\n",
    "}\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.18\n",
    "colors = ['#3498db', '#2ecc71', '#f39c12', '#e74c3c']\n",
    "\n",
    "for i, (metric, values) in enumerate(metrics_values.items()):\n",
    "    ax.bar(x + i*width, values, width, label=metric, color=colors[i])\n",
    "\n",
    "ax.set_xlabel('Model', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Score', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Final Model Performance Comparison', fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1.1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrices for final models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Logistic Regression Confusion Matrix\n",
    "cm_lr_final = confusion_matrix(y_test, y_test_pred_final_lr)\n",
    "sns.heatmap(cm_lr_final, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=target_encoder.classes_, \n",
    "            yticklabels=target_encoder.classes_)\n",
    "axes[0].set_title('Final Logistic Regression\\nConfusion Matrix', fontweight='bold')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "\n",
    "# Random Forest Confusion Matrix\n",
    "cm_rf_final = confusion_matrix(y_test, y_test_pred_final_rf)\n",
    "sns.heatmap(cm_rf_final, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=target_encoder.classes_, \n",
    "            yticklabels=target_encoder.classes_)\n",
    "axes[1].set_title('Final Random Forest\\nConfusion Matrix', fontweight='bold')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification reports\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL LOGISTIC REGRESSION - CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_test_pred_final_lr, target_names=target_encoder.classes_))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RANDOM FOREST - CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(classification_report(y_test, y_test_pred_final_rf, target_names=target_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='9'></a>\n",
    "# 9. Conclusion and Reflection\n",
    "\n",
    "### 9.1 Model Performance Summary\n",
    "\n",
    "This analysis successfully built and evaluated multiple classification models to predict road accident severity:\n",
    "\n",
    "**Models Developed:**\n",
    "1. Neural Network (Multi-Layer Perceptron)\n",
    "2. Logistic Regression (optimized)\n",
    "3. Random Forest Classifier (optimized)\n",
    "\n",
    "**Key Results:**\n",
    "- All models achieved strong performance in predicting accident severity\n",
    "- Feature selection improved model efficiency without sacrificing accuracy\n",
    "- Hyperparameter tuning enhanced model performance across all metrics\n",
    "\n",
    "### 9.2 Impact of Optimization Techniques\n",
    "\n",
    "**Cross-Validation:**\n",
    "- Provided robust estimates of model performance\n",
    "- Helped identify optimal hyperparameters\n",
    "- Reduced overfitting risk\n",
    "\n",
    "**Feature Selection:**\n",
    "- Reduced feature space from ~30 to 20 features\n",
    "- Maintained or improved model performance\n",
    "- Improved model interpretability and training speed\n",
    "- Reduced risk of overfitting\n",
    "\n",
    "### 9.3 Key Insights\n",
    "\n",
    "1. **Important Predictors:** Number of fatalities, injuries, weather conditions, and driver characteristics were among the strongest predictors of accident severity.\n",
    "\n",
    "2. **Model Performance:** Random Forest generally showed slightly better performance due to its ability to capture complex non-linear relationships.\n",
    "\n",
    "3. **Class Balance:** The dataset had relatively balanced classes, which contributed to good performance across all severity levels.\n",
    "\n",
    "### 9.4 Future Directions\n",
    "\n",
    "**Potential Improvements:**\n",
    "1. Explore ensemble methods (e.g., Gradient Boosting, XGBoost)\n",
    "2. Implement SMOTE or other resampling techniques if class imbalance exists\n",
    "3. Feature engineering to create interaction terms\n",
    "4. Deep learning models with more sophisticated architectures\n",
    "5. Collect more granular data (e.g., exact weather measurements, road geometry)\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Deploy model for real-time accident severity prediction\n",
    "- Inform emergency response resource allocation\n",
    "- Support road safety policy development\n",
    "- Contribute to UN SDG 3 targets for reducing traffic fatalities\n",
    "\n",
    "### 9.5 Alignment with UN SDG 3\n",
    "\n",
    "This project directly supports **UN Sustainable Development Goal 3: Good Health and Well-being**, specifically Target 3.6, which aims to halve global deaths and injuries from road traffic accidents by 2030. By accurately predicting accident severity, this model can:\n",
    "\n",
    "- Enable faster emergency response\n",
    "- Inform preventive measures and road safety interventions\n",
    "- Support evidence-based policy making\n",
    "- Contribute to safer road infrastructure planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù End of Notebook\n",
    "\n",
    "**Author:** [Your Name]  \n",
    "**Module:** 5CS037 - Machine Learning Portfolio Project  \n",
    "**Date:** February 2026\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
